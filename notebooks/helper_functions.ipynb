{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helper_functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wZliDSoMkp-"
      },
      "source": [
        "#########################################################################\n",
        "##########################  Helpers for  EDA ############################\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "def visualize_img(df,nrows = 1,ncols = 1): \n",
        "    \"\"\"\n",
        "    This function is used to visualize the images.\n",
        "    \n",
        "    nrows : the number of rows of images\n",
        "    ncols : the number of columns of images\n",
        "    df: dataframe, with columns image id and class label\n",
        "    \n",
        "    return None\n",
        "    \"\"\"\n",
        "    assert (nrows *  ncols >= len(df)),\"Not all the images are shown. Please increase the number of ncols or nrows.\"\n",
        "\n",
        "    labels = {0:'No DR',1:'Mild',2:'Moderate',3:'Severe',4:'Proliferative DR'}\n",
        "    count = 1\n",
        "    plt.figure(figsize=[20,3.4*nrows])\n",
        "    sns.set_style(\"white\")\n",
        "    \n",
        "    df_copy = df.copy().reset_index(drop = True)\n",
        "    for img_id in df_copy['id_code']:\n",
        "      #img_path = cv2.imread('../nput/aptos2019-blindness-detection/train_images/{}.png'.format(img_id))[:,:,::-1]\n",
        "      img_path = cv2.imread(mypath +'/input/aptos2019-blindness-detection/train_images/{}.png'.format(img_id))[:,:,::-1]\n",
        "      plt.subplot(nrows,ncols,count)\n",
        "      plt.imshow(img_path)\n",
        "      plt.title(labels[df_copy.diagnosis[count - 1]],fontsize = 14)\n",
        "      count += 1 \n",
        "    plt.show()\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def rgb2hsv(df):\n",
        "    \"\"\"\n",
        "    This function convert RGb color to HSV color in order to detect low brightness images.\n",
        "    HSV is Hue, Saturation and Value where hue represents the color, Saturation represents the greyness,\n",
        "    and Value represents the brightness.\n",
        "\n",
        "    df: dataframe, with columns image id and class label\n",
        "    \n",
        "    return datafrme\n",
        "    \"\"\"\n",
        "    df_copy = df.copy().reset_index(drop = True)\n",
        "    low_bright = pd.DataFrame(columns = ['id_code','diagnosis'])\n",
        "    \n",
        "    for i in range(len(df_copy)):\n",
        "\n",
        "      img = cv2.imread(mypath + '/input/aptos2019-blindness-detection/train_images/{}.png'.format(df_copy.iloc[i].id_code))\n",
        "      hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "      if hsv[...,2].mean() < 70 and (hsv[...,1].mean() < 90):\n",
        "          low_bright = low_bright.append(df_copy.iloc[i])  \n",
        "            \n",
        "    return low_bright.reset_index(drop = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enhance_bright_contrast(df):\n",
        "    \"\"\"\n",
        "    The function enhances the brightness and contrast of the images and plot\n",
        "    the modified images.\n",
        "\n",
        "    df: dataframe, with columns image id and class label\n",
        "\n",
        "    return None\n",
        "    \"\"\"\n",
        "    \n",
        "    df_copy = df.copy().reset_index(drop = True)\n",
        "    labels = {0:'No DR',1:'Mild',2:'Moderate',3:'Severe',4:'Proliferative DR'}\n",
        "    count = 1\n",
        "    \n",
        "    #1 gives the original image\n",
        "    bright_factor =  1.4\n",
        "    saturation_factor = 1.6\n",
        "    contrast_factor = 1.1\n",
        "    \n",
        "    plt.figure(figsize=[20,3.4*np.ceil(len(df)/5)])\n",
        "    sns.set_style(\"white\")\n",
        "    \n",
        "    for i in range(len(df_copy)):\n",
        "        img = Image.open(mypath + '/input/aptos2019-blindness-detection/train_images/{}.png'.format(df_copy.iloc[i].id_code))\n",
        "        #improve brightness\n",
        "        enhancer1 = ImageEnhance.Brightness(img)\n",
        "        img_output1 = enhancer1.enhance(bright_factor)\n",
        "        enhancer2 = ImageEnhance.Color(img_output1)\n",
        "        img_output2 = enhancer2.enhance(saturation_factor)\n",
        "        enhancer2 = ImageEnhance.Contrast(img_output2)\n",
        "        img_output3 = enhancer2.enhance(contrast_factor)        \n",
        "        plt.subplot(np.ceil(len(df)/5),5,count)\n",
        "        plt.imshow(img_output3)\n",
        "        plt.title(labels[df_copy.diagnosis[count - 1]],fontsize = 14)\n",
        "        count += 1 \n",
        "    plt.show()\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "###########  Helpers for Preprocessing and Modeling #####################\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def warmup_lr_scheduler(epoch, lr):\n",
        "    '''\n",
        "    This function is use to increase the warmup learning rate linearly each iteration.\n",
        "    Assume the total warmup epochs = 5 and the max learning rate = 1e-3.\n",
        "\n",
        "    epochï¼šcurrent epoch\n",
        "    lr: current learning rate \n",
        "\n",
        "    return float number\n",
        "    '''\n",
        "    warmup_epochs = 5\n",
        "    max_lr = 1e-3\n",
        "    increase_rate = max_lr / warmup_epochs\n",
        "    return lr + increase_rate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class f1_score(Callback):\n",
        "    '''\n",
        "    A simple call back for getting the average macro f1_score each step.\n",
        "      \n",
        "    val_data: validation generator\n",
        "    batch_size: model batch size\n",
        "    save_best_mod: whether or not save the model weights as the macro f1_score is higher than before\n",
        "    file_name: define the file name, the file will be over written by the current best macro f1_score\n",
        "\n",
        "    return None\n",
        "    ''' \n",
        "    def __init__(self, val_data, batch_size = 16,save_best_mod = False,file_name = ''):\n",
        "        super().__init__()\n",
        "        self.validation_data = val_data\n",
        "        self.batch_size = batch_size\n",
        "        self.save_best_mod = save_best_mod\n",
        "        self.file_name = file_name\n",
        "    \n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize f1 score and best f1 score of the validation set.'''\n",
        "        #print(self.validation_data)\n",
        "        self.val_f1s = []\n",
        "        self.best_val_f1 = 0\n",
        "        #self.val_recalls = []\n",
        "        #self.val_precisions = []\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Calculate f1 socre at the end of each eopch and save model weights for the best f1 score'''\n",
        "        batches = len(self.validation_data)\n",
        "        val_pred = []\n",
        "        val_true = []\n",
        "        \n",
        "        for batch in range(batches):\n",
        "            xVal, yVal = next(self.validation_data)\n",
        "\n",
        "            val_pred = np.concatenate((val_pred,np.argmax(self.model.predict(xVal),-1)),axis = None)\n",
        "            val_true = np.concatenate((val_true,np.argmax(yVal,-1)),axis = None)\n",
        "        \n",
        "        _val_f1 = f1_score(val_true, val_pred,average = 'macro')\n",
        "        #_val_precision = precision_score(val_true, val_pred)\n",
        "        #_val_recall = recall_score(val_true, val_pred)\n",
        "\n",
        "        if self.save_best_mod == True:\n",
        "          if _val_f1 > self.best_val_f1:\n",
        "            self.model.save_weights(mypath + '/output/model/%s_best_valf1_%.4f.h5'%(file_name,_val_f1))\n",
        "            self.best_val_f1 = _val_f1\n",
        "            print('Best validation macro f1_score: %.4f so far! Saving model...'%(_val_f1))\n",
        "        \n",
        "        self.val_f1s.append(_val_f1)\n",
        "        print(' - val_macro_f1: %f'%(_val_f1))\n",
        "        #self.val_recalls.append(_val_recall)\n",
        "        #self.val_precisions.append(_val_precision)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class QWK_Score(Callback):\n",
        "    '''\n",
        "    A simple call back for getting the QWK score each step.\n",
        "      \n",
        "    val_data: validation generator\n",
        "    batch_size: model batch size\n",
        "    save_best_mod: whether or not save the model weights as the QWK socre is higher than before\n",
        "    file_name: define the file name, the file will be over written by the current best QWK score\n",
        "\n",
        "    return None\n",
        "    ''' \n",
        "    def __init__(self, val_data, batch_size = 16,save_best_mod = False,file_name = ''):\n",
        "        super().__init__()\n",
        "        self.validation_data = val_data\n",
        "        self.batch_size = batch_size\n",
        "        self.save_best_mod = save_best_mod\n",
        "        self.file_name = file_name\n",
        "    \n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize quadratic weighted kappa score and current best quadratic weighted kappa score'''\n",
        "        self.qwk_scores = []\n",
        "        self.best_qwk_score = 0\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        batches = len(self.validation_data)\n",
        "        val_pred = []\n",
        "        val_true = []\n",
        "        \n",
        "        for batch in range(batches):\n",
        "            xVal, yVal = next(self.validation_data)\n",
        "\n",
        "            val_pred = np.concatenate((val_pred,np.argmax(self.model.predict(xVal),-1)),axis = None)\n",
        "            val_true = np.concatenate((val_true,np.argmax(yVal,-1)),axis = None)\n",
        "\n",
        "        _qwk_score = cohen_kappa_score(val_true, val_pred,labels = [0,1,2,3,4],weights = 'quadratic')\n",
        "\n",
        "        if self.save_best_mod == True:\n",
        "          if _qwk_score > self.best_qwk_score:\n",
        "            self.model.save_weights(mypath + '/output/model/%s_best_qwk_%.4f.h5'%(self.file_name,_qwk_score))\n",
        "            self.best_qwk_score = _qwk_score \n",
        "            print('Best qwk_score: %.4f so far! Saving model...'%(_qwk_score))\n",
        "        \n",
        "        self.qwk_scores.append(_qwk_score)\n",
        "        print(' - qwk_score: %f'%(_qwk_score))\n",
        "   \n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    '''\n",
        "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
        "\n",
        "    min_lr: The lower bound of the learning rate range for the experiment.\n",
        "    max_lr: The upper bound of the learning rate range for the experiment.\n",
        "    steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "    epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
        "        \n",
        "    References:\n",
        "    Blog post: jeremyjordan.me/nn-learning-rate\n",
        "    Original paper: https://arxiv.org/abs/1506.01186\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, min_lr=1e-5, max_lr= 2e-3, steps_per_epoch=None, epochs=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.total_iterations = steps_per_epoch * epochs\n",
        "        self.iteration = 0\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        x = self.iteration / self.total_iterations \n",
        "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.iteration += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.iteration)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "            \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        " \n",
        "    def plot_lr(self):\n",
        "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
        "        plt.plot(self.history['iterations'], self.history['lr'])\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Learning rate')\n",
        "        plt.show()\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
        "        plt.plot(self.history['lr'], self.history['loss'])\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_fit_history(fit_hist = []):\n",
        "    '''\n",
        "    This function plots the fit history, including the loss on both of the \n",
        "    training set and validation set along along with the epochs.\n",
        "    \n",
        "    fit_hist: a list of dictionary, where the dictionary is fit history\n",
        "              (note: if there are more than one fit history, pass them in order,from the \n",
        "               earliest to latest.)\n",
        "    \n",
        "    return None\n",
        "    '''\n",
        "    keys  = ['loss','val_loss']\n",
        "\n",
        "    append_fit_hist = defaultdict(list)\n",
        "    for key in keys:\n",
        "      for h in fit_hist:\n",
        "        append_fit_hist[key] += h[key]\n",
        "\n",
        "    total_epoches = len(append_fit_hist[keys[0]])\n",
        "    plt.plot(append_fit_hist['loss'], label='Train loss')\n",
        "    plt.plot(append_fit_hist['val_loss'], label='Validation loss')\n",
        "\n",
        "    plt.xticks(np.arange(total_epoches),np.arange(1,total_epoches + 1))\n",
        "    plt.legend(loc='best')\n",
        "    sns.despine()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}